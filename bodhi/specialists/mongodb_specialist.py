"""
🍃 MongoDB Specialist Agent
==========================

MongoDB specialist that demonstrates NoSQL intelligence:
- Aggregation pipeline generation
- Document-based query creation
- MongoDB-specific optimizations
- Cross-collection operations
"""

import asyncio
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
import random

# MongoDB connectivity
try:
    from pymongo import MongoClient
    MONGO_AVAILABLE = True
except ImportError:
    MONGO_AVAILABLE = False

from ..core.agent import Agent, AgentDNA
from ..core.task import Task
from ..core.result import ExecutionResult

logger = logging.getLogger(__name__)


class MongoDBSpecialist(Agent):
    """MongoDB specialist with aggregation pipeline intelligence"""
    
    def __init__(self, dna: AgentDNA, connection_config: Optional[Dict[str, Any]] = None):
        super().__init__(dna)
        self.connection_config = connection_config or self._get_default_config()
        self.aggregation_patterns = []
        self.query_history = []
        
        # MongoDB operation mappings
        self.mongo_ops = {
            'filter': {'$match': 'Filter documents'},
            'group': {'$group': 'Group documents'},
            'sort': {'$sort': 'Sort documents'},
            'limit': {'$limit': 'Limit results'},
            'project': {'$project': 'Select fields'},
            'lookup': {'$lookup': 'Join collections'},
            'unwind': {'$unwind': 'Flatten arrays'}
        }
        
        logger.info(f"🍃 MongoDB Specialist {self.name} initialized")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Default MongoDB configuration"""
        return {
            'host': 'localhost',
            'port': 27017,
            'database': 'demo',
            'collection': 'documents'
        }
    
    async def _execute_task_logic(self, task: Task) -> ExecutionResult:
        """MongoDB-specific task execution with aggregation pipelines"""
        
        logger.info(f"🍃 MongoDB Specialist executing: {task.intent}")
        
        try:
            natural_query = task.context.input_data.get('natural_query', task.intent)
            
            # Analyze for MongoDB patterns
            mongo_analysis = self._analyze_mongo_query(natural_query)
            
            # Generate aggregation pipeline
            pipeline_result = self._generate_aggregation_pipeline(natural_query, mongo_analysis)
            
            # Mock execution
            execution_result = await self._execute_mongo_query(pipeline_result['pipeline'])
            
            # Learn from execution
            self._learn_from_mongo_execution(natural_query, pipeline_result, execution_result)
            
            return ExecutionResult(
                success=True,
                data={
                    'natural_query': natural_query,
                    'aggregation_pipeline': pipeline_result['pipeline'],
                    'mongo_operations': pipeline_result['operations_used'],
                    'collection_info': pipeline_result.get('collection_info', {}),
                    'execution_result': execution_result,
                    'confidence': pipeline_result.get('confidence', 0.8),
                    'nosql_features': mongo_analysis.get('nosql_features', [])
                },
                agent_id=self.id,
                task_id=task.id,
                message=f"MongoDB aggregation pipeline generated by {self.name}"
            )
            
        except Exception as e:
            logger.error(f"❌ MongoDB execution failed: {str(e)}")
            return ExecutionResult(
                success=False,
                error=str(e),
                agent_id=self.id,
                task_id=task.id
            )
    
    def _analyze_mongo_query(self, natural_query: str) -> Dict[str, Any]:
        """Analyze query for MongoDB-specific patterns"""
        
        query_lower = natural_query.lower()
        
        analysis = {
            'query_type': self._classify_mongo_query_type(query_lower),
            'aggregation_needed': self._needs_aggregation(query_lower),
            'collections_involved': self._identify_collections(query_lower),
            'document_operations': self._identify_document_ops(query_lower),
            'nosql_features': self._suggest_nosql_features(query_lower)
        }
        
        return analysis
    
    def _classify_mongo_query_type(self, query_lower: str) -> str:
        """Classify MongoDB query type"""
        
        if any(term in query_lower for term in ['find', 'get', 'show', 'list']):
            return 'find_documents'
        elif any(term in query_lower for term in ['count', 'total', 'number']):
            return 'count_documents'
        elif any(term in query_lower for term in ['group', 'aggregate', 'sum', 'average']):
            return 'aggregation'
        elif any(term in query_lower for term in ['join', 'lookup', 'combine']):
            return 'multi_collection'
        else:
            return 'complex_aggregation'
    
    def _needs_aggregation(self, query_lower: str) -> bool:
        """Determine if aggregation pipeline is needed"""
        
        aggregation_indicators = [
            'group', 'count', 'sum', 'average', 'max', 'min',
            'sort', 'limit', 'match', 'project', 'unwind'
        ]
        
        return any(indicator in query_lower for indicator in aggregation_indicators)
    
    def _identify_collections(self, query_lower: str) -> List[str]:
        """Identify collections involved in query"""
        
        # Common collection names
        collections = ['users', 'orders', 'products', 'customers', 'documents', 'items']
        
        identified = []
        for collection in collections:
            if collection in query_lower:
                identified.append(collection)
        
        return identified or ['documents']  # Default collection
    
    def _identify_document_ops(self, query_lower: str) -> List[str]:
        """Identify document operations needed"""
        
        operations = []
        
        if any(term in query_lower for term in ['where', 'filter', 'match']):
            operations.append('$match')
        
        if any(term in query_lower for term in ['group', 'categorize']):
            operations.append('$group')
        
        if any(term in query_lower for term in ['sort', 'order']):
            operations.append('$sort')
        
        if any(term in query_lower for term in ['limit', 'top', 'first']):
            operations.append('$limit')
        
        if any(term in query_lower for term in ['select', 'fields', 'only']):
            operations.append('$project')
        
        return operations
    
    def _suggest_nosql_features(self, query_lower: str) -> List[str]:
        """Suggest NoSQL-specific features"""
        
        features = []
        
        if any(term in query_lower for term in ['nested', 'embedded', 'subdocument']):
            features.append('embedded_documents')
        
        if any(term in query_lower for term in ['array', 'list', 'multiple']):
            features.append('array_operations')
        
        if any(term in query_lower for term in ['flexible', 'dynamic', 'schema']):
            features.append('flexible_schema')
        
        if any(term in query_lower for term in ['join', 'lookup', 'reference']):
            features.append('document_references')
        
        return features
    
    def _generate_aggregation_pipeline(self, natural_query: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Generate MongoDB aggregation pipeline"""
        
        pipeline = []
        operations_used = []
        
        # Build pipeline based on analysis
        main_collection = analysis['collections_involved'][0] if analysis['collections_involved'] else 'documents'
        
        # Add match stage for filtering
        if '$match' in analysis['document_operations']:
            match_stage = self._build_match_stage(natural_query)
            pipeline.append(match_stage)
            operations_used.append('$match')
        
        # Add group stage for aggregation
        if analysis['query_type'] in ['aggregation', 'count_documents']:
            group_stage = self._build_group_stage(natural_query, analysis)
            pipeline.append(group_stage)
            operations_used.append('$group')
        
        # Add project stage for field selection
        if '$project' in analysis['document_operations']:
            project_stage = self._build_project_stage(natural_query)
            pipeline.append(project_stage)
            operations_used.append('$project')
        
        # Add sort stage
        if '$sort' in analysis['document_operations']:
            sort_stage = self._build_sort_stage(natural_query)
            pipeline.append(sort_stage)
            operations_used.append('$sort')
        
        # Add limit stage
        if '$limit' in analysis['document_operations']:
            limit_stage = self._build_limit_stage(natural_query)
            pipeline.append(limit_stage)
            operations_used.append('$limit')
        
        # Add lookup for multi-collection queries
        if analysis['query_type'] == 'multi_collection':
            lookup_stage = self._build_lookup_stage(analysis)
            pipeline.append(lookup_stage)
            operations_used.append('$lookup')
        
        return {
            'pipeline': pipeline,
            'operations_used': operations_used,
            'collection': main_collection,
            'confidence': self._calculate_pipeline_confidence(analysis),
            'explanation': self._explain_pipeline(pipeline)
        }
    
    def _build_match_stage(self, natural_query: str) -> Dict[str, Any]:
        """Build $match stage for filtering"""
        
        query_lower = natural_query.lower()
        
        # Simple status matching
        if 'active' in query_lower:
            return {'$match': {'status': 'active'}}
        elif 'inactive' in query_lower:
            return {'$match': {'status': 'inactive'}}
        
        # Date-based matching
        if any(term in query_lower for term in ['recent', 'last', 'today']):
            return {
                '$match': {
                    'created_at': {
                        '$gte': {'$dateSubtract': {'startDate': '$$NOW', 'unit': 'day', 'amount': 30}}
                    }
                }
            }
        
        # Default match all
        return {'$match': {}}
    
    def _build_group_stage(self, natural_query: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Build $group stage for aggregation"""
        
        query_lower = natural_query.lower()
        
        if 'count' in query_lower:
            return {
                '$group': {
                    '_id': None,
                    'total_count': {'$sum': 1}
                }
            }
        
        if 'category' in query_lower or 'group' in query_lower:
            return {
                '$group': {
                    '_id': '$category',
                    'count': {'$sum': 1},
                    'total_value': {'$sum': '$value'}
                }
            }
        
        if any(term in query_lower for term in ['sum', 'total']):
            return {
                '$group': {
                    '_id': None,
                    'total_amount': {'$sum': '$amount'}
                }
            }
        
        if 'average' in query_lower:
            return {
                '$group': {
                    '_id': None,
                    'average_value': {'$avg': '$value'}
                }
            }
        
        # Default grouping
        return {
            '$group': {
                '_id': '$type',
                'count': {'$sum': 1}
            }
        }
    
    def _build_project_stage(self, natural_query: str) -> Dict[str, Any]:
        """Build $project stage for field selection"""
        
        query_lower = natural_query.lower()
        
        if 'name' in query_lower and 'email' in query_lower:
            return {'$project': {'name': 1, 'email': 1, '_id': 0}}
        elif 'name' in query_lower:
            return {'$project': {'name': 1, '_id': 0}}
        elif 'summary' in query_lower:
            return {'$project': {'title': 1, 'summary': 1, 'created_at': 1}}
        
        # Default projection
        return {'$project': {'_id': 1, 'name': 1, 'status': 1}}
    
    def _build_sort_stage(self, natural_query: str) -> Dict[str, Any]:
        """Build $sort stage"""
        
        query_lower = natural_query.lower()
        
        if any(term in query_lower for term in ['newest', 'latest', 'recent']):
            return {'$sort': {'created_at': -1}}
        elif any(term in query_lower for term in ['oldest', 'earliest']):
            return {'$sort': {'created_at': 1}}
        elif 'alphabetical' in query_lower:
            return {'$sort': {'name': 1}}
        
        # Default sort
        return {'$sort': {'_id': 1}}
    
    def _build_limit_stage(self, natural_query: str) -> Dict[str, Any]:
        """Build $limit stage"""
        
        # Extract numbers from query
        import re
        numbers = re.findall(r'\b(\d+)\b', natural_query)
        
        if numbers:
            return {'$limit': int(numbers[0])}
        
        # Qualitative limits
        query_lower = natural_query.lower()
        if 'top' in query_lower or 'first' in query_lower:
            return {'$limit': 10}
        elif 'few' in query_lower:
            return {'$limit': 5}
        
        return {'$limit': 100}  # Default limit
    
    def _build_lookup_stage(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Build $lookup stage for joins"""
        
        collections = analysis['collections_involved']
        
        if len(collections) >= 2:
            return {
                '$lookup': {
                    'from': collections[1],
                    'localField': f'{collections[1][:-1]}_id',  # e.g., user_id
                    'foreignField': '_id',
                    'as': collections[1]
                }
            }
        
        # Default lookup
        return {
            '$lookup': {
                'from': 'related_collection',
                'localField': 'foreign_id',
                'foreignField': '_id',
                'as': 'related_docs'
            }
        }
    
    def _calculate_pipeline_confidence(self, analysis: Dict[str, Any]) -> float:
        """Calculate confidence in generated pipeline"""
        
        confidence = 0.7  # Base confidence
        
        # Increase confidence based on clear patterns
        if analysis['document_operations']:
            confidence += 0.1
        
        if analysis['nosql_features']:
            confidence += 0.1
        
        if analysis['query_type'] != 'complex_aggregation':
            confidence += 0.1
        
        return min(1.0, confidence)
    
    def _explain_pipeline(self, pipeline: List[Dict[str, Any]]) -> str:
        """Generate human-readable pipeline explanation"""
        
        explanations = []
        
        for stage in pipeline:
            stage_name = list(stage.keys())[0]
            
            if stage_name == '$match':
                explanations.append("Filter documents based on criteria")
            elif stage_name == '$group':
                explanations.append("Group documents and calculate aggregations")
            elif stage_name == '$project':
                explanations.append("Select specific fields")
            elif stage_name == '$sort':
                explanations.append("Sort documents")
            elif stage_name == '$limit':
                explanations.append("Limit number of results")
            elif stage_name == '$lookup':
                explanations.append("Join with related collection")
        
        return ' → '.join(explanations)
    
    async def _execute_mongo_query(self, pipeline: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Execute MongoDB aggregation pipeline (mock)"""
        
        await asyncio.sleep(0.1)  # Simulate execution
        
        # Mock MongoDB result
        mock_result = {
            'success': True,
            'documents': [
                {'_id': '507f1f77bcf86cd799439011', 'name': 'Document 1', 'status': 'active'},
                {'_id': '507f1f77bcf86cd799439012', 'name': 'Document 2', 'status': 'active'},
                {'_id': '507f1f77bcf86cd799439013', 'name': 'Document 3', 'status': 'inactive'}
            ],
            'document_count': 3,
            'execution_time_ms': 25,
            'stages_executed': len(pipeline),
            'index_usage': 'Used index on status field',
            'mongo_version': '6.0'
        }
        
        return mock_result
    
    def _learn_from_mongo_execution(self, natural_query: str, pipeline_result: Dict[str, Any], 
                                   execution_result: Dict[str, Any]):
        """Learn from MongoDB execution"""
        
        learning_entry = {
            'timestamp': datetime.now(timezone.utc),
            'natural_query': natural_query,
            'pipeline': pipeline_result['pipeline'],
            'operations_used': pipeline_result['operations_used'],
            'confidence': pipeline_result['confidence'],
            'execution_success': execution_result.get('success', False),
            'execution_time': execution_result.get('execution_time_ms', 0),
            'document_count': execution_result.get('document_count', 0)
        }
        
        self.query_history.append(learning_entry)
        
        # Limit history
        if len(self.query_history) > 50:
            self.query_history = self.query_history[-25:]
        
        logger.info(f"📚 MongoDB learning recorded - {len(self.query_history)} queries in history")
    
    def __str__(self) -> str:
        return f"MongoDBSpecialist({self.name}, pipelines={len(self.query_history)})" 